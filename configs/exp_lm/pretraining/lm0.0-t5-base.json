{
    "experiment":{
        "id": "lm0.0-t5-base",
        "type": "pretraining",
        "description": "pretrain t5-base on paraphrase generation",
        "seed": 42,
        "output_dir": "results/lm"
    },

    "data": {
        "directory": "data/linearized_domain",
        "labeled":{
            "source": {
                "train": "gyafc/src/train.json",
                "dev": "gyafc/src/dev.json",
                "test": "gyafc/src/test.json"
            },
            "target": {
                "train": "gyafc/tgt/train.json",
                "dev": "gyafc/tgt/dev.json",
                "test": "gyafc/tgt/test.json"
            },
            "columns": ["tokens", "labels"]
        }
    },

    "config": {
        "model_name_or_path": "t5-base",
        "src_task_prefix": "transfer formal to informal: ",
        "tgt_task_prefix": "transfer informal to formal: "
    },

    "tokenizer": {
        "model_name_or_path": "t5-base",
        "new_tokens": ["<START_PERSON>", "<END_PERSON>", "<START_NORP>", "<END_NORP>", "<START_FAC>", "<END_FAC>", "<START_ORG>", "<END_ORG>", 
            "<START_GPE>", "<END_GPE>", "<START_LOC>", "<END_LOC>", "<START_PRODUCT>", "<END_PRODUCT>", "<START_EVENT>", "<END_EVENT>", 
            "<START_WORK_OF_ART>", "<END_WORK_OF_ART>", "<START_LAW>", "<END_LAW>", "<START_LANGUAGE>", "<END_LANGUAGE>", "<START_DATE>", 
            "<END_DATE>", "<START_TIME>", "<END_TIME>", "<START_PERCENT>", "<END_PERCENT>", "<START_MONEY>", "<END_MONEY>", "<START_QUANTITY>", 
            "<END_QUANTITY>", "<START_ORDINAL>", "<END_ORDINAL>", "<START_CARDINAL>", "<END_CARDINAL>"],
        "use_fast": false,
        "model_max_length": 512,
        "label_pad_token_id": -100,
        "ignore_pad_token_for_loss": true
    },

    "model": {
        "model_class": "DAT5PreTrainedModel",
        "model_name_or_path": "t5-base",
        "resume_from_checkpoint": false
    },

    "optim": {
        "learning_rate": 3e-4,
        "num_train_epochs": 5,
        "max_steps": -1,
        "per_gpu_train_batch_size": 32,
        "per_gpu_eval_batch_size": 24,
        "gradient_accumulation_steps": 1,
        "weight_decay": 0.01,
        "beta_1": 0.9,
        "beta_2": 0.999,
        "adam_epsilon": 1e-8,
        "max_grad_norm": 1.0,
        "warmup_steps": 0
    }
}